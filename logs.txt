10:58:38,803 etl_logger INFO PySpark Session Created!
10:58:44,368 etl_logger INFO Schema check OK.
10:58:54,985 etl_logger INFO Data rows are identical!
11:24:23,218 etl_logger INFO PySpark Session Created!
11:24:28,563 etl_logger INFO Schema check OK.
11:24:39,295 etl_logger INFO Data rows are identical!
09:38:16,626 etl_logger INFO PySpark Session Created!
09:38:33,259 etl_logger INFO Schema check OK.
09:38:40,235 etl_logger INFO Data rows are identical!
09:48:29,218 etl_logger INFO PySpark Session Created!
09:48:44,877 etl_logger INFO Schema check OK.
09:48:51,964 etl_logger INFO Data rows are identical!
15:02:16,789 etl_logger INFO PySpark Session Created!
15:02:50,547 etl_logger INFO Schema check OK.
15:03:12,204 etl_logger INFO Data rows are identical!
11:45:48,655 etl_logger INFO PySpark Session Created!
11:46:03,208 etl_logger INFO Schema check OK.
11:46:14,47 etl_logger INFO Data rows are identical!
11:46:14,68 py4j.clientserver INFO Closing down clientserver connection
14:22:58,676 etl_logger INFO PySpark Session Created!
14:23:06,469 etl_logger INFO Schema check OK.
14:23:22,135 etl_logger INFO Data rows are identical!
14:23:22,155 py4j.clientserver INFO Closing down clientserver connection
14:57:59,141 etl_logger INFO PySpark Session Created!
14:58:15,784 py4j.clientserver INFO Error while receiving.
Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=1716>
14:58:15,786 py4j.clientserver INFO Closing down clientserver connection
14:58:15,786 root ERROR Exception while sending command.
Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=1716>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
14:58:15,789 py4j.clientserver INFO Error while receiving.
Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 362, in signal_handler
    self.cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 1447, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\sql\utils.py", line 190, in deco
    return f(*a, **kw)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o16.sc
14:58:15,823 py4j.clientserver INFO Closing down clientserver connection
14:58:15,823 root ERROR Exception while sending command.
Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 362, in signal_handler
    self.cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 1447, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\sql\utils.py", line 190, in deco
    return f(*a, **kw)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o16.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
14:58:15,824 py4j.clientserver INFO Closing down clientserver connection
14:58:15,856 py4j.clientserver INFO Closing down clientserver connection
15:42:14,484 etl_logger INFO PySpark Session Created!
15:42:27,664 py4j.clientserver INFO Closing down clientserver connection
15:44:37,903 etl_logger INFO PySpark Session Created!
15:44:49,900 py4j.clientserver INFO Closing down clientserver connection
16:28:34,551 etl_logger INFO PySpark Session Created!
16:28:45,318 py4j.clientserver INFO Closing down clientserver connection
17:00:04,425 etl_logger INFO PySpark Session Created!
17:00:11,942 py4j.clientserver INFO Closing down clientserver connection
17:01:24,538 etl_logger INFO PySpark Session Created!
17:01:32,45 py4j.clientserver INFO Closing down clientserver connection
09:40:12,398 etl_logger INFO PySpark Session Created!
09:40:21,47 botocore.credentials INFO Found credentials in environment variables.
09:40:24,454 py4j.clientserver INFO Closing down clientserver connection
09:43:36,514 etl_logger INFO PySpark Session Created!
09:44:13,747 etl_logger INFO PySpark Session Created!
09:49:17,803 etl_logger INFO PySpark Session Created!
09:49:18,830 py4j.clientserver INFO Error while sending or receiving.
Traceback (most recent call last):
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
09:49:18,830 py4j.clientserver INFO Error while receiving.
Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
09:49:18,832 py4j.clientserver INFO Closing down clientserver connection
09:49:18,833 py4j.clientserver INFO Closing down clientserver connection
09:49:18,833 root INFO Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
09:49:18,834 root ERROR Exception while sending command.
Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
09:49:18,836 py4j.clientserver INFO Closing down clientserver connection
09:49:20,874 py4j.clientserver INFO Closing down clientserver connection
09:49:20,874 py4j.clientserver INFO Closing down clientserver connection
09:49:20,875 py4j.clientserver INFO Closing down clientserver connection
09:49:20,875 py4j.clientserver INFO Error while receiving.
Traceback (most recent call last):
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 362, in signal_handler
    self.cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 1447, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1320, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
09:49:20,876 py4j.clientserver INFO Closing down clientserver connection
09:49:20,876 root ERROR Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Oleksandr Forkun\AppData\Local\Programs\Python\Python39\lib\socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 362, in signal_handler
    self.cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\pyspark\context.py", line 1447, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1320, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Spark\spark-3.3.0-bin-hadoop2\python\lib\py4j-0.10.9.5-src.zip\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
09:49:20,877 py4j.clientserver INFO Closing down clientserver connection
09:49:20,877 py4j.clientserver INFO Closing down clientserver connection
10:13:18,643 etl_logger INFO PySpark Session Created!
10:13:31,839 py4j.clientserver INFO Closing down clientserver connection
10:27:22,467 etl_logger INFO PySpark Session Created!
10:27:35,613 py4j.clientserver INFO Closing down clientserver connection
10:29:06,87 etl_logger INFO PySpark Session Created!
10:29:18,729 botocore.credentials INFO Found credentials in environment variables.
10:29:20,889 py4j.clientserver INFO Closing down clientserver connection
10:40:05,979 etl_logger INFO PySpark Session Created!
10:40:18,289 botocore.credentials INFO Found credentials in environment variables.
10:40:20,233 botocore.credentials INFO Found credentials in environment variables.
10:40:23,293 py4j.clientserver INFO Closing down clientserver connection
10:43:37,305 etl_logger INFO PySpark Session Created!
10:43:37,708 botocore.credentials INFO Found credentials in environment variables.
10:43:40,982 py4j.clientserver INFO Closing down clientserver connection
10:58:43,189 etl_logger INFO PySpark Session Created!
10:58:43,587 botocore.credentials INFO Found credentials in environment variables.
10:59:08,337 py4j.clientserver INFO Closing down clientserver connection
11:00:26,426 etl_logger INFO PySpark Session Created!
11:00:26,827 botocore.credentials INFO Found credentials in environment variables.
11:00:51,502 py4j.clientserver INFO Closing down clientserver connection
11:06:11,812 etl_logger INFO PySpark Session Created!
11:06:12,190 botocore.credentials INFO Found credentials in environment variables.
11:06:36,867 py4j.clientserver INFO Closing down clientserver connection
11:07:06,811 etl_logger INFO PySpark Session Created!
11:07:07,264 botocore.credentials INFO Found credentials in environment variables.
11:07:31,927 py4j.clientserver INFO Closing down clientserver connection
11:10:15,294 etl_logger INFO PySpark Session Created!
11:10:15,710 botocore.credentials INFO Found credentials in environment variables.
11:10:40,420 py4j.clientserver INFO Closing down clientserver connection
11:12:39,48 etl_logger INFO PySpark Session Created!
11:12:39,455 botocore.credentials INFO Found credentials in environment variables.
11:12:43,109 py4j.clientserver INFO Closing down clientserver connection
11:21:49,71 etl_logger INFO PySpark Session Created!
11:21:49,473 botocore.credentials INFO Found credentials in environment variables.
11:22:14,558 py4j.clientserver INFO Closing down clientserver connection
11:39:17,49 etl_logger INFO PySpark Session Created!
11:39:17,485 botocore.credentials INFO Found credentials in environment variables.
11:39:42,292 py4j.clientserver INFO Closing down clientserver connection
11:49:08,340 etl_logger INFO PySpark Session Created!
11:49:08,744 botocore.credentials INFO Found credentials in environment variables.
11:49:33,505 py4j.clientserver INFO Closing down clientserver connection
11:55:30,27 etl_logger INFO PySpark Session Created!
11:55:30,430 botocore.credentials INFO Found credentials in environment variables.
11:55:34,765 py4j.clientserver INFO Closing down clientserver connection
11:56:51,781 etl_logger INFO PySpark Session Created!
11:56:52,183 botocore.credentials INFO Found credentials in environment variables.
11:56:56,190 py4j.clientserver INFO Closing down clientserver connection
12:02:12,804 etl_logger INFO PySpark Session Created!
12:02:13,204 botocore.credentials INFO Found credentials in environment variables.
12:02:17,307 botocore.credentials INFO Found credentials in environment variables.
12:02:33,550 py4j.clientserver INFO Closing down clientserver connection
12:23:25,705 etl_logger INFO PySpark Session Created!
12:23:26,107 botocore.credentials INFO Found credentials in environment variables.
12:23:30,234 botocore.credentials INFO Found credentials in environment variables.
12:23:41,636 awswrangler.redshift ERROR {'S': 'ERROR', 'C': 'XX000', 'M': 'Spectrum Scan Error', 'D': '\n  -----------------------------------------------\n  error:  Spectrum Scan Error\n  code:      15007\n  context:   Unmatched number of columns between table and file. Table columns: 7, Data columns: 8, File name: https://s3.eu-west-1.amazonaws.com/redbucket337/nova.parquet/da2d334fa6bd4fc0a43ad6a8f22a4f2b_0.snappy.parquet\n  query:     16657\n  location:  dory_util.cpp:1377\n  process:   worker_thread [pid=27742]\n  -----------------------------------------------\n', 'F': '../src/sys/xen_execute.cpp', 'L': '10520', 'R': 'pg_throw'}
12:23:43,383 py4j.clientserver INFO Closing down clientserver connection
12:25:37,227 etl_logger INFO PySpark Session Created!
12:25:37,692 botocore.credentials INFO Found credentials in environment variables.
12:25:41,877 botocore.credentials INFO Found credentials in environment variables.
12:25:45,760 awswrangler.redshift ERROR {'S': 'ERROR', 'C': 'XX000', 'M': 'Spectrum Scan Error', 'D': '\n  -----------------------------------------------\n  error:  Spectrum Scan Error\n  code:      15007\n  context:   Unmatched number of columns between table and file. Table columns: 7, Data columns: 8, File name: https://s3.eu-west-1.amazonaws.com/redbucket337/nova.parquet/77f73196c5ea49c3bf2efc5533e687fb_0.snappy.parquet\n  query:     16683\n  location:  dory_util.cpp:1377\n  process:   worker_thread [pid=28965]\n  -----------------------------------------------\n', 'F': '../src/sys/xen_execute.cpp', 'L': '10520', 'R': 'pg_throw'}
12:25:47,492 py4j.clientserver INFO Closing down clientserver connection
12:26:25,404 etl_logger INFO PySpark Session Created!
12:26:25,944 botocore.credentials INFO Found credentials in environment variables.
12:26:30,539 botocore.credentials INFO Found credentials in environment variables.
12:26:34,448 awswrangler.redshift ERROR {'S': 'ERROR', 'C': 'XX000', 'M': 'Spectrum Scan Error', 'D': '\n  -----------------------------------------------\n  error:  Spectrum Scan Error\n  code:      15007\n  context:   Unmatched number of columns between table and file. Table columns: 7, Data columns: 8, File name: https://s3.eu-west-1.amazonaws.com/redbucket337/nova.parquet/5655ae76f86c412c9ef344b2c1c28bbe_0.snappy.parquet\n  query:     16696\n  location:  dory_util.cpp:1377\n  process:   worker_thread [pid=25212]\n  -----------------------------------------------\n', 'F': '../src/sys/xen_execute.cpp', 'L': '10520', 'R': 'pg_throw'}
12:26:36,173 py4j.clientserver INFO Closing down clientserver connection
12:30:25,595 etl_logger INFO PySpark Session Created!
12:30:25,969 botocore.credentials INFO Found credentials in environment variables.
12:30:30,28 botocore.credentials INFO Found credentials in environment variables.
12:30:38,544 py4j.clientserver INFO Closing down clientserver connection
12:38:31,1 etl_logger INFO PySpark Session Created!
12:38:31,401 botocore.credentials INFO Found credentials in environment variables.
12:38:35,420 botocore.credentials INFO Found credentials in environment variables.
12:38:50,831 py4j.clientserver INFO Closing down clientserver connection
15:19:42,44 etl_logger INFO PySpark Session Created!
15:19:42,448 botocore.credentials INFO Found credentials in environment variables.
15:19:45,837 py4j.clientserver INFO Closing down clientserver connection
15:21:42,790 etl_logger INFO PySpark Session Created!
15:21:43,214 botocore.credentials INFO Found credentials in environment variables.
15:21:47,945 botocore.credentials INFO Found credentials in environment variables.
15:21:56,581 py4j.clientserver INFO Closing down clientserver connection
